---
title: "HW 6"
author: "Joshua Zhong"
date: "`r Sys.Date()`"
output: word_document
---

```{r options, echo = TRUE, include = FALSE}
    
    #library(tidyverse)
    #library(dplyr)
    library(ggplot2)
    library(GGally)
    #library(knitr)
    #library(kableExtra)

    knitr::opts_chunk$set(
    	echo = TRUE,
    	message = FALSE,
    	warning = FALSE,
    	comment = NA,
    	results = TRUE,
    	digits = 4
    )

```


```{r q1}

setwd("C:/STA419")
hw6 <- read.csv(file = "HW6data.csv")
  
```


## Question 2

### Part A

Using ggplot, we create a scatterplot between LBM vs Ht. We add a line of best fit using OLS to the plot as well.   

```{r part a}

ggplot(hw6, aes(x = Ht, y = LBM)) +
  geom_point() +
  geom_smooth(method = lm)

lm.fit <- lm(LBM~Ht, data = hw6)
  

```




### Part B

Running simple linear regression, we get our coefficients. The results we get are that LBM = -129.095 + 1.077 * Ht. This means that with a height of 0 centimeters, the estimated lean body mass is -129.094 (our y-intercept). And for each incremental gain of 1 cm in height, the lean body mass increases by 1.077. 

```{r part b}
  
  summary(lm.fit)


```


### Part C

Now, we include RCC (or red cell count) into our linear regression and conduct a multiple linear regression. Based off the multiple linear regression, we get the equation LBM = -143.558 + 0.931 * Ht + 8.620 * RCC. This means that for every 1 cm increase in height with all other explanatory variables held constant, lean body mass increases by 0.931. Likewise, for every 1 unit increase in red cell count with all other explanatory variables held constant, lean body mass increases by 8.620. 

```{r part c}

  lm.fit.2 <- lm(LBM~Ht+RCC, data = hw6)
  summary(lm.fit.2)


```



### Part D

Adding Sex to our linear regression, we now have 3 explanatory variables. Because Sex is a dichotomous categorical variable, there will be one dummy variable. R by default chooses the baseline value by the factor level in alphabetical order, meaning that Sex = Male is our one dummy variable and Sex = Female is our baseline variable. 

The equation we get is LBM = -86.021 + 0.743 * Ht + 2.560 * RCC + 10.067 * Sexm. This means that for every 1 cm increase in height with all other explanatory variables held constant, lean body mass increases by 0.743. Likewise, for every 1 unit increase in red cell count with all other explanatory variables held constant, lean body mass increases by 2.560. Finally to interpret the dummy variable, 10.067 represents the difference in an athlete's lean body mass when comparing a male athlete and female athlete with all other factors the exact same; specifically, a male athlete will have a lean body mass of 10.067 higher than a female athlete when both athletes have the same height and red cell count. 

```{r part d}
  unique(hw6$Sex)

  lm.fit.3 <- lm(LBM~Ht+RCC+Sex, data = hw6)
  summary(lm.fit.3)

```



### Part E

Next, we plot the scatterplot of lean body mass vs height with sex been colored different colors. Looking at the scatterplot, I would argue that there seems to be a slight interaction effect between Sex and Ht. The reason why is that the lines of best fit aren't exactly parallel, meaning that the effect of height on lean body mass isn't necessarily consistent across sexes. However, because the linear regression lines have 95% confidence intervals for the slopes that overlap significantly, the slopes are not significantly different from one another, meaning there's not enough evidence that interaction effect is present. 

```{r part e}

ggplot(hw6, aes(x =Ht, y =LBM,color = Sex)) +
  geom_point( ) +
  geom_smooth(method = lm)

```



### Part F

Using the predict() function to predict the lean body mass of a female athlete that is 180 cm tall and has a red blood cell count of 4.5, we get that their estimated LBM is 59.153. 

```{r part f}

p1 <- predict(lm.fit.3, newdata = data.frame(Ht = 180, RCC = 4.5, Sex = "f"))
p1

  
```



### Part G

Using the predict() function to predict the lean body mass of a female athlete that is 181 cm tall and has a red blood cell count of 4.5, we get that their estimated LBM is 59.896.

Computing the difference between these 2 predictions, we get a lean body mass of 0.74254. Looking back at the summary() function outputs from part D, we see that this matches the coefficient for Ht. This makes intuitive sense because our LBM predictions differed by the matter of a 1 cm increase in Ht and the Ht summary coefficient represents the change in LBM when height is increased by 1 cm with all other factors held constant. 

```{r part g}


p2 <- predict(lm.fit.3, newdata = data.frame(Ht = 181, RCC = 4.5, Sex = "f"))
p2

p2-p1

  
```


### Part H

Next, we use 5-fold cross validation to estimate the test RMSE of our full linear model. 5-fold cross validation indicates that we randomly split the entire data set into 5 disparate subsets then take turns using 1 of the 5 subsets as a testing set and the remaining 4 subsets as a training set. Finally, we average over these 5 folds. 

To assess test error, we look at the RMSE value. We achieved an RMSE of 2.746. The lower the RMSE, the better the fit. Since the summary statistics for our response variable LBM show a unimodal distribution with no skews as well as a standard deviation of 13.070, our RMSE of 2.746 is small and represents a good fit. 

```{r part h}

library(caret)
set.seed(1)

fitControl <- trainControl(method = "cv", number = 5)

model.5cv <- train(LBM ~ ., 
                  data = hw6, 
                  method = "lm",
                  trControl = fitControl)
model.5cv

summary(hw6$LBM)
sd(hw6$LBM)
  
```

 

### Part I

We check all odd values of our tuning parameter k from 1 to 19 inclusive, but without centering or scaling the variables. The optimal value for k is the k with the lowest RMSE - which is k = 5 with an RMSE of 4.688. 

Our model performed worse than linear regression as linear regression produced a lower test error (RMSE = 2.746) than our k-Nearest Neighbors with k = 5 (RMSE = 4.688). This most likely happened because KNN has higher variance than linear regression as a byproduct of being a nonparametric model and not making strong assumptions. 

```{r part i}
set.seed(1)
kGrid <- expand.grid(k = seq(1, 19, by = 2))



model.knn <- train(LBM ~ ., 
                      data = hw6, 
                      method = "knn",
                      trControl = fitControl,
                      tuneGrid = kGrid)

model.knn

```


### Part J

Now we perform KNN, but with the variables centered and scaled. Our test RMSEs performed better across the board, although now k = 3 is the optimal tuning parameter with an RMSE of 3.789.

If we look at the standard deviations of our numeric variables it makes intuitive sense that scaling the variables would decrease RMSE. The reasoning is that RCC has a substantially lower standard deviation than the other variables - which would disproportionately weigh the variable less than the other numeric variables when all variables are non-scaled. This would inherently increase error in any non-scaled models. 

```{r part j}
set.seed(1)
model.knn.sc <- train(LBM ~ ., 
                      data = hw6, 
                      method = "knn",
                      preProc = c("center", "scale"),
                      trControl = fitControl,
                      tuneGrid = kGrid)

model.knn.sc


table(hw6$Sex)
hw6.vars <- as.list(hw6)
hw6.vars$Sex <- NULL
lapply(hw6.vars, sd)

```




















